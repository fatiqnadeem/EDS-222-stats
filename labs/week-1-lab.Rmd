---
title: "EDS 222: Week 1: In-class Assignment"
author: "{STUDENT NAME}"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# Load all the packages needed here
library(tidyverse)
library(readr)
```

# Sampling in R

```{r}
# We are going to learn two types of sampling methodologies here: (i) Simple Random Sampling and; (ii) Stratified Sampling. For this we will make use of the county level data from the usdata package. This data frame contains data from counties in all 50 states plus the District of Columbia. 

# Step 1: Use the `library(usdata)` to load the data. The data will be stored in an object called county. Use the functions `filter()` and `droplevels()` to remove all data from the District of Columbia and store it in a tidy dataframe called `county_noDC`

library(usdata)
county_noDC <- 
  county %>%
  filter(state != "District of Columbia") %>%
  droplevels()

# Step 2: You are tasked at the EPA to collect data on air quality from all 50 states in the US. However, the current president has initiated significant budget cuts in the EPA which only allows them to collect data from 150 of the over 3000 counties in the United States. Use the function `slice_sample()` to select 150 counties that you think can be representative of the US. and store them in a tidy dataframe called `county_srs`

county_srs <- 
  county_noDC %>%
  slice_sample(n = 150)

# Comments for Fatiq: Students need to learn how to create a sampling function and how it works i.e. probabilities etc. Maybe start with sample()? Tidyverse makes life very easy. -_-

# Step 3: Evaluate the distribution of sampled counties across different states in the dataframe `county_srs`. Are the number of sampled counties in each state different? Would you call this a representative sample? 

county_srs %>%
  group_by(state) %>%
  count() %>% 
  ungroup()

# Step 4: As you might have seen in step 3 that the number of counties in each state was different and that makes our sample non-representative of the US (why?). Use the concepts of stratified random sampling learned in class to select equal number of counties in each state. Hint: This code is very similar to how we selected the simple random sample, except that before sampling we first group by state and then sample three observations per group. Store the resulting dataframe as `country_str`

county_str <- 
  county_noDC %>%
  group_by(state) %>%
  slice_sample(n = 3) %>% 
  ungroup()
```


# Sampling issues in real data: How does sampling influence your data and what you can learn from it?

```{r}
# As we have learned in the lectures that population and sample data frames differ. In this exercise we will look at a case study from the developing world. South Asia has one of the most polluted cities in the world. Pakistan was ranked to contain the second most air pollution in the world in 2020 (IQAIR, 2020). We use data from Pakistan that is generated from a crowd-sourced data source (IQAir) and a public-second data source i.e. Environmental Protection Department, Government of Punjab, Pakistan. The crowsourced data source has data for the years 2018 and 2019 for multiple cities in Pakistan. The government data has data for the years 2018 and 2019 for one city. Morover, the government data has only one observation per day whereas the crowdsourced data has multiple ovservations per day. 

# Step 1: Identify by just reading the description above which dataframe is the sample data and which is the population data. 

# Answer: The goverrnment data is the sample data and the crowdsourced data is the population data. 

# Step 2: Use the following links to load the data into R, name the government data as `govt` and the crowdsourced data as `crowsourced`: 

# crowdsourced data: https://www.dropbox.com/s/8o9n7yhb5nituk8/airpol-PK-crowdsourced.csv?dl=0
# government data: https://www.dropbox.com/s/t5f01jj0d6nev37/airpol-PK-govt.csv?dl=0


# Note for Fatiq: fix the code below to work for a dropbox link
crowdsourced <- read_delim("~/Dropbox/EDS222_data/week-one/lab/airpol-PK-crowdsourced.csv", ";", escape_double = FALSE, trim_ws = TRUE)
govt <- read_delim("~/Dropbox/EDS222_data/week-one/lab/airpol-PK-govt.csv", ";", escape_double = FALSE, trim_ws = TRUE)
  
# Step 3: Sampling strategy: 

  # Step 3.1: Merge the two data frames using the common denominator date and keep the aqi variables both from the sample and the population data
  # Hint: use the left_join() function in tidyverse for this task.

merged_df <- 
  crowdsourced %>% 
  group_by(date) %>% 
  select(date, aqi_crowdsourced = aqi) %>% 
  summarise_all(mean, na.rm = TRUE) %>% 
  ungroup() %>% 
  inner_join(govt, by = "date") %>% 
  rename(aqi_govt = aqi)

# Step 4 : # The AQI represents the air quality index defined the Environmental Protection Agency (details can be found here:       https://www.epa.gov/outdoor-air-quality-data/about-air-data-reports#aqi). Use the variables AQI to calculate mean, min, max and standard deviation for the sample data, population data and the difference between the sample and population data

merged_df %>% 
  mutate(difference = aqi_crowdsourced - aqi_govt) %>% 
  summarise(mean(aqi_crowdsourced), 
            mean(aqi_govt), 
            mean(difference),
            
            min(aqi_crowdsourced), 
            min(aqi_govt), 
            min(difference),
            
            max(aqi_crowdsourced), 
            max(aqi_govt), 
            max(difference),
            
            sd(aqi_crowdsourced), 
            sd(aqi_govt), 
            sd(difference)) %>% 
  gather()


# Step 5: Identify biases between sample and population:
# Discuss how the sampling strategy biases (or doesnâ€™t) the parameters in Step 4 as approximations of population statistics 



```


# Designing an experiment

```{r}
# 1. Generate a data frame with 100 observations and potential outcomes, control
  # and treatment, control has random noise, the treatment variable has the 
  # random noise from the control plus a treatment effect. 
sample <-
  tibble(Y0 = rnorm(n = 100),
         Y1 = Y0 + 0.5)

# 2. Assign binary treatment with equal prob of being in control and treatment:
sample <-
  sample %>%
  mutate(Z = sample(
    c(0, 1),
    size = 100,
    replace = TRUE,
    prob = c(0.5, 0.5)
  ))

# 3. Contruct observed outcome
sample <-
  sample %>%
  mutate(
    Y = if_else(Z == 1, Y1, Y0))

# 4. Look at data, treatment effect:    
    head(sample)
    
    sample %>% summarize(ATE = mean(Y1 - Y0))
    
    sample %>% summarize(dim = mean(Y[Z == 1]) - mean(Y[Z == 0]))
    
    sample %>% summarize(
      estimand = mean(Y1 - Y0),
      estimate = mean(Y[Z == 1]) - mean(Y[Z == 0]))
    
    
# 5. Do steps 1-4 in one pipeline:
    tibble(Y0 = rnorm(n = 100), Y1 = Y0 + 0.5) %>%
      mutate(Z = sample(
        c(0, 1), 100, replace = TRUE, prob = c(.5, .5))) %>%
      mutate(Y = if_else(Z == 1, Y1, Y0)) %>%
      summarize(
        estimand = mean(Y1 - Y0),
        estimate = mean(Y[Z == 1]) - mean(Y[Z == 0]))
    
 # 6. Create a function to do steps 1-4:    
    run_experiment <- function() {
      fixed_population %>%
        mutate(Z = sample(c(0, 1), 100, replace = TRUE, prob = c(.5, .5))) %>%
        mutate(Y = if_else(Z == 1, Y1, Y0)) %>%
        summarize(estimand = mean(Y1 - Y0),
                  estimate = mean(Y[Z == 1]) - mean(Y[Z == 0])) %>%
        as_vector()
    }

# 7. Using the function in 6 to do 1000 replications of the process in 1-4 and report what you observe about the results:     
    many_runs <-
      replicate(
        n = 1000,
        expr = run_experiment(),
        simplify = TRUE)
    
    apply(many_runs, 1, mean)
    
# 8. add blocking within the sampling part: 
    
    # Step 1: First we need to identify what blocking is and why is it needed. For this we need to see what is the bias that our experiment suffers from due to lack of blocking. To do this, design an experiment, randomize the treatment status and see the bias in the simulated variable. Prove that in over 10,000 iterations this bias is reduced to 0. 
    
    tibble(u = rnorm(100),  
           x = rnorm(100)) %>%  
      mutate(Z = sample(rep(c(0, 1), each = n() / 2), n(), replace = FALSE)) %>% 
      summarize(balance = mean(x[Z == 1]) - mean(x[Z == 0])) %>%
      pull(balance)
    
    my_experiment <- function() {
      tibble(u = rnorm(100),  
             x = rnorm(100)) %>%  
        mutate(Z = sample(rep(c(0, 1), each = n() / 2), n(), replace = FALSE)) %>% 
        summarize(balance = mean(x[Z == 1]) - mean(x[Z == 0])) %>%
        pull(balance)
      }
    
    balance_statistics <- replicate(10000, my_experiment(), simplify = TRUE)
    mean(balance_statistics)
    
    # Step 2: Show the iterated bias generated in step 1 in a plot
    ggplot(data.frame(balance = balance_statistics), aes(balance)) + geom_histogram()
    
    # Step 3: Now we need to think about blocking. We will start to do this by changing the function to use block randomization. For this, first, we should construct blocks. First generate one observed covariate and construct blocks according to that. 
    
    # Step 3.1: we need to create a block indicator. Blocks are based on observed units being similar to each other in the greatest way possible. Make use of the function `arrange()` to sort the data in the order of the observed covariate and then mutate a block indicator where the the data resembles a series where the first two units are in block 1, the second two are in block 2, etc.

    # Step 3.2: Now, once we have blocks, we need to randomize all the units within the blocks where one unit will go to treatment and one will go to control. randomize *within* blocks, exactly one to treatment and one to control. 
    
    # Note for Fatiq: The code below is ridocoulosy difficult. Please make it easier while not using any prepackaged functions. 

    blocking <-
      function(data) {
        data$Z <- NA
        for (b in unique(data$block)) {
          data$Z[data$block == b] <-
            sample(c(0, 1), size = 2, replace = FALSE)
        }
        data
      }
    
    my_experiment <- function() {
      tibble(u = rnorm(100), 
             x = rnorm(100)) %>%  
        mutate(Y0 = x + u,
               Y1 = Y0 + 5) %>%
        arrange(x) %>%
        mutate(block = rep(1:(n() / 2), each = 2)) %>% 
        block_ra() %>%
        mutate(Y = if_else(Z == 0, Y0, Y1))
    }

```
